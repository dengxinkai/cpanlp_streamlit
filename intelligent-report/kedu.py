import streamlit as st
import numpy as np
import pandas as pd
import tempfile
import re
import time
import pinecone
from typing import List, Union,Callable,Dict, Optional, Any
from langchain.prompts import PromptTemplate
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain import LLMChain
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback
from langchain.vectorstores import Pinecone
import asyncio

st.set_page_config(
    page_title="ChatReport",
    page_icon="https://raw.githubusercontent.com/dengxinkai/cpanlp_streamlit/main/app/%E6%9C%AA%E5%91%BD%E5%90%8D.png",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://www.cpanlp.com/',
        'Report a bug': "https://www.cpanlp.com/",
        'About': "æ™ºèƒ½æŠ¥å‘Š"
    }
)


logo_url = "https://raw.githubusercontent.com/dengxinkai/cpanlp_streamlit/main/app/%E6%9C%AA%E5%91%BD%E5%90%8D.png"
with st.sidebar:
    st.image(logo_url,width=150)
    st.subheader("ğŸ‘‡ :blue[ç¬¬ä¸€æ­¥ï¼šè¾“å…¥ OpenAI API å¯†é’¥]")
    if 'input_api' in st.session_state:
        st.text_input("api-key",st.session_state["input_api"], key="input_api")
    else:
        st.info('è¯·å…ˆè¾“å…¥æ­£ç¡®çš„OpenAI API å¯†é’¥')
        st.text_input('api-key','', key="input_api")
    with st.expander("ChatOpenAIå±æ€§é…ç½®"):
        temperature = st.slider("`temperature`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬éšæœºæ€§å’Œå¤šæ ·æ€§çš„å‚æ•°ã€‚è¾ƒé«˜çš„æ¸©åº¦å€¼é€šå¸¸é€‚ç”¨äºç”Ÿæˆè¾ƒä¸ºè‡ªç”±æµç•…çš„æ–‡æœ¬ï¼Œè€Œè¾ƒä½çš„æ¸©åº¦å€¼åˆ™é€‚ç”¨äºç”Ÿæˆæ›´åŠ ç¡®å®šæ€§çš„æ–‡æœ¬ã€‚")
        frequency_penalty = st.slider("`frequency_penalty`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬ä¸­å•è¯é‡å¤é¢‘ç‡çš„æŠ€æœ¯ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹å¯¹å•è¯é‡å¤ä½¿ç”¨çš„æƒ©ç½šå°±è¶Šä¸¥æ ¼ï¼Œç”Ÿæˆæ–‡æœ¬ä¸­å‡ºç°ç›¸åŒå•è¯çš„æ¦‚ç‡å°±è¶Šä½ï¼›æ•°å€¼è¶Šå°ï¼Œç”Ÿæˆæ–‡æœ¬ä¸­å‡ºç°ç›¸åŒå•è¯çš„æ¦‚ç‡å°±è¶Šé«˜ã€‚")
        presence_penalty = st.slider("`presence_penalty`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶è¯­è¨€ç”Ÿæˆæ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶å¯¹è¾“å…¥æç¤ºçš„é‡è§†ç¨‹åº¦çš„å‚æ•°ã€‚presence_penaltyçš„å€¼è¾ƒä½ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å¯èƒ½ä¸è¾“å…¥æç¤ºéå¸¸æ¥è¿‘ï¼Œä½†ç¼ºä¹åˆ›æ„æˆ–åŸåˆ›æ€§ã€‚presence_penaltyè®¾ç½®ä¸ºè¾ƒé«˜çš„å€¼ï¼Œæ¨¡å‹å¯èƒ½ç”Ÿæˆæ›´å¤šæ ·åŒ–ã€æ›´å…·åŸåˆ›æ€§ä½†ä¸è¾“å…¥æç¤ºè¾ƒè¿œçš„æ–‡æœ¬ã€‚")
        top_p = st.slider("`top_p`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œè¾ƒå°çš„top_på€¼ä¼šè®©æ¨¡å‹é€‰æ‹©çš„è¯æ›´åŠ ç¡®å®šï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¼šæ›´åŠ ä¸€è‡´ï¼Œè€Œè¾ƒå¤§çš„top_på€¼åˆ™ä¼šè®©æ¨¡å‹é€‰æ‹©çš„è¯æ›´åŠ å¤šæ ·ï¼Œç”Ÿæˆçš„æ–‡æœ¬åˆ™æ›´åŠ å¤šæ ·åŒ–ã€‚")
        model = st.radio("`æ¨¡å‹é€‰æ‹©`",
                                ("gpt-3.5-turbo",
                                "gpt-4"),
                                index=0,key="main_model")
    with st.expander("å‘é‡æ•°æ®åº“é…ç½®"):
        chunk_size = st.number_input('chunk_size',value=800,min_value=200,max_value=2500,step=100,key="chunk_size",help='æ¯ä¸ªæ–‡æœ¬æ•°æ®å—çš„å¤§å°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå°†chunk_sizeè®¾ç½®ä¸º1000ï¼Œåˆ™å°†è¾“å…¥æ–‡æœ¬æ•°æ®åˆ†æˆ1000ä¸ªå­—ç¬¦çš„å—ã€‚')
        chunk_overlap = st.number_input('chunk_overlap',value=0,min_value=0,max_value=500,step=50,key="chunk_overlap",help='æ¯ä¸ªæ–‡æœ¬æ•°æ®å—ä¹‹é—´é‡å çš„å­—ç¬¦æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå°†chunk_overlapè®¾ç½®ä¸º200ï¼Œåˆ™ç›¸é‚»çš„ä¸¤ä¸ªå—å°†æœ‰200ä¸ªå­—ç¬¦çš„é‡å ã€‚è¿™å¯ä»¥ç¡®ä¿åœ¨å—ä¹‹é—´æ²¡æœ‰ä¸¢å¤±çš„æ•°æ®ï¼ŒåŒæ—¶è¿˜å¯ä»¥é¿å…é‡å¤å¤„ç†ç›¸é‚»å—ä¹‹é—´çš„æ•°æ®ã€‚')
        top_k = st.number_input('top_k',value=3,min_value=0,max_value=10,step=1,key="top_k",help="ç”¨äºæ§åˆ¶æŸ¥è¯¢çš„ç»“æœæ•°é‡ï¼ŒæŒ‡å®šä»æ•°æ®åº“ä¸­è¿”å›çš„ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„å‰ k ä¸ªå‘é‡")
    st.write(":red[ä½¿ç”¨æ³¨æ„äº‹é¡¹ï¼š]")
    st.write("1ã€ä¸Šä¼ æ–‡æ¡£ï¼šä½¿ç”¨ç³»ç»Ÿä¸Šä¼ åŠŸèƒ½å°†pdfæ–‡æ¡£ä¸Šä¼ è‡³è‡ªå»ºå‘é‡æ•°æ®åº“ã€‚")
    st.write("2ã€AIæŸ¥è¯¢ï¼šä½¿ç”¨AIåŠŸèƒ½å¯¹æ•°æ®åº“è¿›è¡ŒæŸ¥è¯¢ï¼Œè·å¾—æ‰€éœ€æ•°æ®ã€‚")
    st.warning("è¯·åŠæ—¶æ¸…ç†ä¸å†éœ€è¦çš„æ•°æ®åº“ï¼Œä»¥ä¾¿ä»–äººä½¿ç”¨ã€‚")
   

@st.cache_data(persist="disk")
def convert_df(df):
   return df.to_csv(index=False).encode('utf-8')
st.warning("è®°å¾—ç»å¸¸ä½¿ç”¨åˆ·æ–°å’Œæ¸…é™¤ç¼“å­˜åŠŸèƒ½")
if st.button('åˆ·æ–°é¡µé¢',key="rerun"):
    st.experimental_rerun()
if st.button('æ¸…é™¤æ‰€æœ‰ç¼“å­˜',key="clearcache"):
    st.cache_data.clear()

st.subheader("ğŸ‘‡ :blue[ç¬¬äºŒæ­¥ï¼šåˆ›å»ºè‡ªå·±çš„æ•°æ®åº“æˆ–è¿æ¥åˆ°å·²æœ‰æ•°æ®åº“]")
pinename = st.text_input('**æ•°æ®åº“åç§°**','report',key="pinename",help="è¯·æ³¨æ„ï¼Œç³»ç»Ÿæ¯æ—¥å®šæœŸæ¸…é™¤æ•°æ®åº“")

pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                      environment="us-west1-gcp-free", 
                      namespace=pinename
                      )
index = pinecone.Index(index_name="kedu")
st.warning("åˆ«å¿˜äº†åˆ é™¤ä¸å†ä½¿ç”¨çš„æ•°æ®åº“")
if st.button('åˆ é™¤æ•°æ®åº“',key="deletepine"):
    index = pinecone.Index(index_name="kedu")
    index.delete(deleteAll='true', namespace=pinename)
if st.session_state.input_api:
    embeddings_cho = OpenAIEmbeddings(openai_api_key=st.session_state.input_api)
    llm=ChatOpenAI(
        model_name=model,
        temperature=temperature,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        top_p=top_p,
        openai_api_key=st.session_state.input_api
    )
    def upload_file(input_text):
        loader = PyPDFLoader(input_text)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
        texts = text_splitter.split_documents(documents)
        Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
#     @st.cache_resource
    def upload_file_pdf():
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            tmp_file.write(file.read())
            tmp_file.flush()
            loader = PyPDFLoader(tmp_file.name)
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
            )
            texts = text_splitter.split_documents(documents)
            Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
            st.success(f"Uploaded {len(texts)} documents from pdf file.")
            st.cache_data.clear()
    def upload_file_pptx():
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            tmp_file.write(file.read())
            tmp_file.flush()
            loader = UnstructuredPowerPointLoader(tmp_file.name)
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
            )
            texts = text_splitter.split_documents(documents)
            Pinecone.from_documents(texts, embeddings_cho, index_name="kedu",namespace=pinename)
            st.success(f"Uploaded {len(texts)} documents from pptx file.")
            st.cache_data.clear()
    do_question=[]
    do_answer=[]
    st.subheader("ğŸ‘‡:blue[ç¬¬ä¸‰æ­¥ï¼šé€‰æ‹©æ•°æ®åº“æ–‡ä»¶ä¸Šä¼ æ–¹å¼]")
    fileoption = st.radio('**æ•°æ®åº“åˆ›å»ºæ–¹å¼**',('æœ¬åœ°ä¸Šä¼ ', 'URL'),key="fileoption")
    def upload_query(input_file):
        ww=""
        pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
              environment="us-west1-gcp-free", 
              namespace=pinename
              )
        index = pinecone.Index(index_name="kedu")
        a=embeddings_cho.embed_query(input_file)
        www=index.query(vector=a, top_k=top_k, namespace=pinename, include_metadata=True)
        for i in range(top_k):
            ww+=www["matches"][i]["metadata"]["text"]
        return ww
 #ä¸Šä¼  
    upload_file_pptx()
    with get_openai_callback() as cb:
        if fileoption=="æœ¬åœ°ä¸Šä¼ ":
            file = st.file_uploader("PDFä¸Šä¼ ", type='pptx' ,key="upload_files")
            if file is not None:
                with st.spinner('Wait for it...'):
                    upload_file_pptx()
                
        else:
            input_text = st.text_input('PDFç½‘å€', 'http://static.cninfo.com.cn/finalpage/2023-04-29/1216712300.PDF',key="pdfweb",help="ä¾‹å­")
            if st.button('è½½å…¥æ•°æ®åº“',key="pdfw"):
                with st.spinner('Wait for it...'):
                    pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                          environment="us-west1-gcp-free", 
                          namespace=pinename
                          )
                    index = pinecone.Index(index_name="kedu")
                    upload_file(input_text)
                    st.cache_data.clear()
#ä¸»è¦åŠŸèƒ½                
        input_file = st.text_input('**æŸ¥è¯¢**','å…¬å¸æ ¸å¿ƒç«äº‰åŠ›',key="file_web",help="ä¾‹å­")
        st.warning("ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢åªéœ€è¦é€šè¿‡ API æ¥å£è·å–åµŒå…¥å‘é‡ï¼Œè€Œä¸éœ€è¦è¿›è¡Œå…¶ä»– API è°ƒç”¨ï¼Œä½†ä½¿ç”¨ AI æŸ¥è¯¢éœ€è¦ä½¿ç”¨ API æ¥å£ï¼Œå¹¶ä¸”ä¼šäº§ç”Ÿä¸€å®šè´¹ç”¨ã€‚")
        if st.button('æ•°æ®åº“æŸ¥è¯¢',key="file_upload"):
            ww=upload_query(input_file)
            st.success(ww)
            do_question.append(input_file)
            do_answer.append(ww)

        if st.button('AIæŸ¥è¯¢',key="aifile_upload",type="primary"):
            with st.spinner('Wait for it...'):
                ww=""
                pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                      environment="us-west1-gcp-free", 
                      namespace='ceshi'
                      )
                index = pinecone.Index(index_name="kedu")
                start_time = time.time()
                a=embeddings_cho.embed_query(input_file)
                www=index.query(vector=a, top_k=top_k, namespace=pinename, include_metadata=True)
                for i in range(top_k):
                    ww+=www["matches"][i]["metadata"]["text"]
                template = """Use the following portion of a long document to see if any of the text is relevant to answer the question. 
                Return any relevant text verbatim.
                Respond in Chinese.
                QUESTION: {question}
                =========
                {summaries}
                =========
                FINAL ANSWER IN CHINESE:"""
                prompt = PromptTemplate(
                    input_variables=["summaries", "question"],
                    template=template,
                )
                chain = LLMChain(prompt=prompt, llm=llm)

                ww1=chain.predict(summaries=ww, question=input_file)
                st.success(ww1)
                do_question.append(input_file)
                do_answer.append(ww1)
                end_time = time.time()
                elapsed_time = end_time - start_time
                with st.expander("è´¹ç”¨"):
                        st.success(f"Total Tokens: {cb.total_tokens}")
                        st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                        st.success(f"Completion Tokens: {cb.completion_tokens}")
                        st.success(f"Total Cost (USD): ${cb.total_cost}")
                st.write(f"é¡¹ç›®å®Œæˆæ‰€éœ€æ—¶é—´: {elapsed_time:.2f} ç§’")  

        input_files = st.text_input('**æ‰¹é‡æŸ¥è¯¢**','#å…¬å¸åç§°#å…¬å¸äº§å“',key="file_webss",help="ä¸åŒé—®é¢˜ç”¨#éš”å¼€ï¼Œæ¯”å¦‚ï¼šå…¬å¸æ”¶å…¥#å…¬å¸åç§°#å…¬å¸å‰æ™¯")
        if st.button('æ•°æ®åº“æ‰¹é‡æŸ¥è¯¢',key="file_uploads1"):
            input_list = re.split(r'#', input_files)[1:]
            async def upload_all_files_async(input_list):
                do_question, do_answer = [], []

                tasks = []
                for input_file in input_list:
                    task = asyncio.create_task(upload_query_async(input_file))
                    tasks.append(task)
                results = await asyncio.gather(*tasks)
                for key, inter_result in zip(input_list, results):
                    st.write(key)
                    st.success(inter_result)
                    do_question.append(key)
                    do_answer.append(inter_result)
                return do_question,do_answer
            async def upload_query_async(input_file):
                ww=""
                pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                              environment="us-west1-gcp-free", 
                              namespace=pinename
                              )
                index = pinecone.Index(index_name="kedu")
                a=embeddings_cho.embed_query(input_file)
                www=index.query(vector=a, top_k=top_k, namespace=pinename, include_metadata=True)
                for i in range(top_k):
                    ww+=www["matches"][i]["metadata"]["text"]
                return ww
            do_question, do_answer=asyncio.run(upload_all_files_async(input_list))

        if st.button('AIæ‰¹é‡æŸ¥è¯¢',key="aifile_uploadss",type="primary"):
            with st.spinner('Wait for it...'):

                start_time = time.time()
                input_list = re.split(r'#', input_files)[1:]
                async def upload_all_files_async(input_list):
                    do_question, do_answer = [], []
                    tasks = []
                    for input_file in input_list:
                        task = asyncio.create_task(upload_query_async(input_file))
                        tasks.append(task)
                    results = await asyncio.gather(*tasks)
                    for key, inter_result in zip(input_list, results):
                        st.write(key)
                        st.success(inter_result)
                        do_question.append(key)
                        do_answer.append(inter_result)
                    return do_question,do_answer
                async def upload_query_async(input_file):
                    ww=""
                    ww1=""
                    pinecone.init(api_key="1ebbc1a4-f41e-43a7-b91e-24c03ebf0114",  # find at app.pinecone.io
                          environment="us-west1-gcp-free", 
                          namespace='ceshi'
                          )
                    index = pinecone.Index(index_name="kedu")
                    start_time = time.time()
                    a=embeddings_cho.embed_query(input_file)
                    www=index.query(vector=a, top_k=top_k, namespace=pinename, include_metadata=True)
                    for i in range(top_k):
                        ww+=www["matches"][i]["metadata"]["text"]
                    template = """Use the following portion of a long document to see if any of the text is relevant to answer the question. 
                    Return any relevant text verbatim.
                    Respond in Chinese.
                    QUESTION: {question}
                    =========
                    {summaries}
                    =========
                    FINAL ANSWER IN CHINESE:"""
                    prompt = PromptTemplate(
                        input_variables=["summaries", "question"],
                        template=template,
                    )
                    chain = LLMChain(prompt=prompt, llm=llm)
                    ww1=chain.predict(summaries=ww, question=input_file)
                    return ww1
                do_question, do_answer=asyncio.run(upload_all_files_async(input_list))
                end_time = time.time()
                elapsed_time = end_time - start_time
                st.write(f"é¡¹ç›®å®Œæˆæ‰€éœ€æ—¶é—´: {elapsed_time:.2f} ç§’")  
                with st.expander("è´¹ç”¨"):
                        st.success(f"Total Tokens: {cb.total_tokens}")
                        st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                        st.success(f"Completion Tokens: {cb.completion_tokens}")
                        st.success(f"Total Cost (USD): ${cb.total_cost}")

        df_inter = pd.DataFrame({
            'é—®é¢˜':do_question,
            'å›ç­”':do_answer,
             })
        with st.expander("å›ç­”è®°å½•"):
            st.dataframe(df_inter, use_container_width=True)
        csv_inter = convert_df(df_inter)
        st.download_button(
           "ä¸‹è½½å›ç­”è®°å½•",
           csv_inter,
           "file.csv",
           "text/csv",
           key='download-csv_inter'
        )
        
            
else:
    st.header("è¯·å…ˆè¾“å…¥æ­£ç¡®çš„Openai api-key")
    
