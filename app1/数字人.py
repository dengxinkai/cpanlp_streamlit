import streamlit as st
import asyncio
import faiss
import numpy as np
import pandas as pd
import re
import time
import math
from datetime import datetime, timedelta
from pydantic import BaseModel, Field
from typing import List, Union,Callable,Dict, Optional, Any, Tuple
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from langchain.schema import BaseLanguageModel,Document
from langchain.docstore import InMemoryDocstore
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback
from langchain.retrievers import TimeWeightedVectorStoreRetriever
import boto3
st.set_page_config(
    page_title="æ•°å­—äºº",
    page_icon="https://raw.githubusercontent.com/dengxinkai/cpanlp_streamlit/main/app1/shuziren.jpg",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://www.cpanlp.com/',
        'Report a bug': "https://www.cpanlp.com/",
        'About': 'ç¤¾ç§‘å®éªŒæ•°å­—äºº'
    }
)
@st.cache_resource
def load_digitalaws():
    dynamodb = boto3.client(
        'dynamodb',
        region_name="ap-southeast-1", 
        aws_access_key_id=st.secrets["AWS_ACCESS_KEY_ID"],
        aws_secret_access_key=st.secrets["AWS_SECRET_ACCESS_KEY"]
        )
    table_name = 'digit_human1'
    response = dynamodb.scan(
        TableName=table_name
    )
    items = response['Items']
    dfaws = pd.DataFrame(items)
    return dfaws
@st.cache_data(persist="disk")
def convert_df(df):
   return df.to_csv(index=False).encode('utf-8')
with st.sidebar:
    st.image("https://raw.githubusercontent.com/dengxinkai/cpanlp_streamlit/main/app1/shuziren.jpg")
    with st.expander("ğŸ‘‡ :blue[**ç¬¬ä¸€æ­¥ï¼šè¾“å…¥ OpenAI API å¯†é’¥**]"):
        if 'input_api' in st.session_state:
            st.text_input(st.session_state["input_api"], key="input_api",label_visibility="collapsed")
        else:
            st.info('è¯·å…ˆè¾“å…¥æ­£ç¡®çš„openai api-key')
            st.text_input('api-key','', key="input_api",type="password")
        temperature = st.slider("`temperature`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬éšæœºæ€§å’Œå¤šæ ·æ€§çš„å‚æ•°ã€‚è¾ƒé«˜çš„æ¸©åº¦å€¼é€šå¸¸é€‚ç”¨äºç”Ÿæˆè¾ƒä¸ºè‡ªç”±æµç•…çš„æ–‡æœ¬ï¼Œè€Œè¾ƒä½çš„æ¸©åº¦å€¼åˆ™é€‚ç”¨äºç”Ÿæˆæ›´åŠ ç¡®å®šæ€§çš„æ–‡æœ¬ã€‚")
        frequency_penalty = st.slider("`frequency_penalty`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬ä¸­å•è¯é‡å¤é¢‘ç‡çš„æŠ€æœ¯ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹å¯¹å•è¯é‡å¤ä½¿ç”¨çš„æƒ©ç½šå°±è¶Šä¸¥æ ¼ï¼Œç”Ÿæˆæ–‡æœ¬ä¸­å‡ºç°ç›¸åŒå•è¯çš„æ¦‚ç‡å°±è¶Šä½ï¼›æ•°å€¼è¶Šå°ï¼Œç”Ÿæˆæ–‡æœ¬ä¸­å‡ºç°ç›¸åŒå•è¯çš„æ¦‚ç‡å°±è¶Šé«˜ã€‚")
        presence_penalty = st.slider("`presence_penalty`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶è¯­è¨€ç”Ÿæˆæ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶å¯¹è¾“å…¥æç¤ºçš„é‡è§†ç¨‹åº¦çš„å‚æ•°ã€‚presence_penaltyçš„å€¼è¾ƒä½ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å¯èƒ½ä¸è¾“å…¥æç¤ºéå¸¸æ¥è¿‘ï¼Œä½†ç¼ºä¹åˆ›æ„æˆ–åŸåˆ›æ€§ã€‚presence_penaltyè®¾ç½®ä¸ºè¾ƒé«˜çš„å€¼ï¼Œæ¨¡å‹å¯èƒ½ç”Ÿæˆæ›´å¤šæ ·åŒ–ã€æ›´å…·åŸåˆ›æ€§ä½†ä¸è¾“å…¥æç¤ºè¾ƒè¿œçš„æ–‡æœ¬ã€‚")
        top_p = st.slider("`top_p`", 0.01, 0.99, 0.3,help="ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œè¾ƒå°çš„top_på€¼ä¼šè®©æ¨¡å‹é€‰æ‹©çš„è¯æ›´åŠ ç¡®å®šï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¼šæ›´åŠ ä¸€è‡´ï¼Œè€Œè¾ƒå¤§çš„top_på€¼åˆ™ä¼šè®©æ¨¡å‹é€‰æ‹©çš„è¯æ›´åŠ å¤šæ ·ï¼Œç”Ÿæˆçš„æ–‡æœ¬åˆ™æ›´åŠ å¤šæ ·åŒ–ã€‚")
        model = st.radio("`æ¨¡å‹é€‰æ‹©`",
                                ("gpt-3.5-turbo",
                                "gpt-4"),
                                index=0)
    USER_NAME = st.text_input("è¯·å¡«å†™åˆ›æ•°äººå§“å","Person", key="user_name")
agent_keys = [key for key in st.session_state.keys() if key.startswith('agent')]   
if st.button('åˆ·æ–°é¡µé¢'):
    st.experimental_rerun()
    st.cache_data.clear()
if agent_keys:
    do_name=[]
    do_age=[]
    do_gender=[]
    do_traits=[]
    do_status=[]
    do_reflection_threshold=[]
    do_memory=[]
    do_summary=[]
    with st.expander("å½“å‰æ•°å­—äººï¼š"):
        for i,key in enumerate(agent_keys):
            y=st.session_state[key]
            col1, col2, col3 = st.columns([2, 1,6])
            with col1:
                st.write(f"{i+1}ã€",y.name)
                do_name.append(y.name)
                do_age.append(y.age)
                do_gender.append(y.gender)
                do_traits.append(y.traits)
                do_status.append(y.status)
                do_reflection_threshold.append(y.reflection_threshold)
                do_memory.append(y.agent_memory)
                do_summary.append(y.summary)
            with col2:
                if st.button('åˆ é™¤',key=f"del_{key}"):
                    del st.session_state[key]
                    st.experimental_rerun()
            with col3:        
                if st.button('æ€»ç»“',help="æ€»ç»“",key=f"sum_{key}",type="primary"):
                    start_time = time.time()
                    with get_openai_callback() as cb:
                        st.success(st.session_state[key].get_summary())
                        st.success(f"Total Tokens: {cb.total_tokens}")
                        st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                        st.success(f"Completion Tokens: {cb.completion_tokens}")
                        st.success(f"Total Cost (USD): ${cb.total_cost}")
                    end_time = time.time()
                    st.write(f"é‡‡è®¿ç”¨æ—¶ï¼š{round(end_time-start_time,2)} ç§’")
        df = pd.DataFrame({
                        'å§“å': do_name,
                        'å¹´é¾„': do_age,
                        'æ€§åˆ«': do_gender,
                        'ç‰¹å¾': do_traits,
                        'çŠ¶æ€': do_status,
                        'åæ€é˜ˆå€¼': do_reflection_threshold,
                        'è®°å¿†':do_memory,
                        'æ€»ç»“':do_summary
                    })

        st.dataframe(df, use_container_width=True)
        if st.button('æ€»ç»“æ‰€æœ‰æ•°å­—äºº',help="æ€»ç»“æ‰€æœ‰",key=f"sum_all",type="primary"):
                    start_time = time.time()
                    with get_openai_callback() as cb:
                        async def summary_all_agents(agent_keys):
                            tasks = []
                            for key in agent_keys:
                                task = asyncio.create_task(get_summary_async(st.session_state[key]))
                                tasks.append(task)
                            results = await asyncio.gather(*tasks)
                            for key, summary in zip(agent_keys, results):
                                st.success(summary)
                        async def get_summary_async(agent):
                            summary = await asyncio.to_thread(agent.get_summary, force_refresh=True)
                            return summary
                        asyncio.run(summary_all_agents(agent_keys))
                        st.success(f"Total Tokens: {cb.total_tokens}")
                        st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                        st.success(f"Completion Tokens: {cb.completion_tokens}")
                        st.success(f"Total Cost (USD): ${cb.total_cost}")
                    end_time = time.time()
                    st.write(f"é‡‡è®¿ç”¨æ—¶ï¼š{round(end_time-start_time,2)} ç§’")
        if st.button('åˆ é™¤æ‰€æœ‰æ•°å­—äºº',key=f"delete_all"):
            for i,key in enumerate(agent_keys):
                del st.session_state[key]
            st.experimental_rerun()
        csv = convert_df(df)
        st.download_button(
           "ä¸‹è½½æ•°å­—äºº",
           csv,
           "file.csv",
           "text/csv",
           key='download-csv'
        )

else:
    st.warning("å½“å‰ä¸å­˜åœ¨æ•°å­—äºº") 
tab1, tab2, tab3,tab4 = st.tabs(["æ•°å­—äººåˆ›å»º", "æ–°è§‚å¯Ÿä¸è®°å¿†", ":blue[**ç¤¾ç§‘è°ƒæŸ¥**]", "æ•°å­—äººå¯¹è¯"])
LLM = ChatOpenAI(
        model_name=model,
        temperature=temperature,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        top_p=top_p,
        openai_api_key=st.session_state.input_api
    ) 
agents={}
class GenerativeAgent(BaseModel):
    name: str
    age: int
    gender: str
    traits: str
    status: str
    llm: BaseLanguageModel
    memory_retriever: TimeWeightedVectorStoreRetriever
    verbose: bool = False
    agent_memory: str= ""
    reflection_threshold: Optional[float] = None
    
    current_plan: List[str] = []    
    summary: str = ""  #: :meta private:
    summary_refresh_seconds: int= 3600  #: :meta private:
    last_refreshed: datetime =Field(default_factory=datetime.now)  #: :meta private:
    daily_summaries: List[str] = [] #: :meta private:
    memory_importance: float = 0.0 #: :meta private:
    max_tokens_limit: int = 1200 #: :meta private:
    class Config:
        arbitrary_types_allowed = True
    @staticmethod
    def _parse_list(text: str) -> List[str]:
        lines = re.split(r'\n', text.strip())
        return [re.sub(r'^\s*\d+\.\s*', '', line).strip() for line in lines]
    def _compute_agent_summary(self):
        """"""
        prompt = PromptTemplate.from_template(
            "How would you summarize {name}'s core characteristics given the"
            +" following statements:\n"
            +"{related_memories}"
            + "Do not embellish."
            +"\n\nSummary: "
            +"è¾“å‡ºç”¨ä¸­æ–‡"
        )
        # The agent seeks to think about their core characteristics.
        relevant_memories = self.fetch_memories(f"{self.name}'s core characteristics")
        relevant_memories_str = "\n".join([f"{mem.page_content}" for mem in relevant_memories])
        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)
        return chain.run(name=self.name, related_memories=relevant_memories_str).strip()
    
    def _get_topics_of_reflection(self, last_k: int = 50) -> Tuple[str, str, str]:
        """Return the 3 most salient high-level questions about recent observations."""
        prompt = PromptTemplate.from_template(
            "{observations}\n\n"
            + "Given only the information above, what are the 3 most salient"
            + " high-level questions we can answer about the subjects in the statements?"
            + " Provide each question on a new line.\n\n"
                        +"è¾“å‡ºç”¨ä¸­æ–‡"
        )
        reflection_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)
        observations = self.memory_retriever.memory_stream[-last_k:]
        observation_str = "\n".join([o.page_content for o in observations])
        result = reflection_chain.run(observations=observation_str)
        return self._parse_list(result)
    
    def _get_insights_on_topic(self, topic: str) -> List[str]:
        """Generate 'insights' on a topic of reflection, based on pertinent memories."""
        prompt = PromptTemplate.from_template(
            "Statements about {topic}\n"
            +"{related_statements}\n\n"
            + "What 5 high-level insights can you infer from the above statements?"
            + " (example format: insight (because of 1, 5, 3))"
                        +"è¾“å‡ºç”¨ä¸­æ–‡ï¼Œé™¤äº†å…³é”®è¯"
        )
        related_memories = self.fetch_memories(topic)
        related_statements = "\n".join([f"{i+1}. {memory.page_content}" 
                                        for i, memory in 
                                        enumerate(related_memories)])
        reflection_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)
        result = reflection_chain.run(topic=topic, related_statements=related_statements)
        return self._parse_list(result)
    def pause_to_reflect(self) -> List[str]:
        print(f"Character {self.name} is reflecting")
        new_insights = []
        topics = self._get_topics_of_reflection()
        for topic in topics:
            insights = self._get_insights_on_topic( topic)
            for insight in insights:
                self.add_memory(insight)
            new_insights.extend(insights)
        return new_insights
    def _score_memory_importance(self, memory_content: str, weight: float = 0.15) -> float:
        prompt = PromptTemplate.from_template(
         "On the scale of 1 to 10, where 1 is purely mundane"
         +" (e.g., brushing teeth, making bed) and 10 is"
         + " extremely poignant (e.g., a break up, college"
         + " acceptance), rate the likely poignancy of the"
         + " following piece of memory. Respond with a single integer."
         + "\nMemory: {memory_content}"
         + "\nRating: "
        )
        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)
        score = chain.run(memory_content=memory_content).strip()
        match = re.search(r"^\D*(\d+)", score)
        if match:
            return (float(score[0]) / 10) * weight
        else:
            return 0.0
    def add_memory(self, memory_content: str) -> List[str]:
        """Add an observation or memory to the agent's memory."""
        importance_score = self._score_memory_importance(memory_content)
        self.memory_importance += importance_score
        document = Document(page_content=memory_content, metadata={"importance": importance_score})
        result = self.memory_retriever.add_documents([document])
        if (self.reflection_threshold is not None 
            and self.memory_importance > self.reflection_threshold
            and self.status != "Reflecting"):
            old_status = self.status
            self.status = "Reflecting"
            self.pause_to_reflect()
            # Hack to clear the importance from reflection
            self.memory_importance = 0.0
            self.status = old_status
        return result
    def fetch_memories(self, observation: str) -> List[Document]:
        return self.memory_retriever.get_relevant_documents(observation)
    def get_summary(self, force_refresh: bool = False) -> str:
        current_time = datetime.now()
        since_refresh = (current_time - self.last_refreshed).seconds
        if not self.summary or since_refresh >= self.summary_refresh_seconds or force_refresh:
            self.summary = self._compute_agent_summary()
            self.last_refreshed = current_time
        return (
            f"{self.summary}"
        )
    def get_full_header(self, force_refresh: bool = False) -> str:
        summary = self.get_summary(force_refresh=force_refresh)
        current_time_str =  datetime.now().strftime("%B %d, %Y, %I:%M %p")
        return f"{summary}\nIt is {current_time_str}.\n{self.name}'s status: {self.status}"
    def _get_entity_from_observation(self, observation: str) -> str:
        prompt = PromptTemplate.from_template(
            "What is the observed entity in the following observation? {observation}"
            +"\nEntity="
        )
        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)
        return chain.run(observation=observation).strip()
    def _get_entity_action(self, observation: str, entity_name: str) -> str:
        prompt = PromptTemplate.from_template(
            "What is the {entity} doing in the following observation? {observation}"
            +"\nThe {entity} is"
        )
        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)
        return chain.run(entity=entity_name, observation=observation).strip()
    def _format_memories_to_summarize(self, relevant_memories: List[Document]) -> str:
        content_strs = set()
        content = []
        for mem in relevant_memories:
            if mem.page_content in content_strs:
                continue
            content_strs.add(mem.page_content)
            created_time = mem.metadata["created_at"].strftime("%B %d, %Y, %I:%M %p")
            content.append(f"- {created_time}: {mem.page_content.strip()}")
        return "\n".join([f"{mem}" for mem in content])
    def summarize_related_memories(self, observation: str) -> str:
        """Summarize memories that are most relevant to an observation."""
        entity_name = self._get_entity_from_observation(observation)
        entity_action = self._get_entity_action(observation, entity_name)
        q1 = f"What is the relationship between {self.name} and {entity_name}"
        relevant_memories = self.fetch_memories(q1) # Fetch memories related to the agent's relationship with the entity
        q2 = f"{entity_name} is {entity_action}"
        relevant_memories += self.fetch_memories(q2) # Fetch things related to the entity-action pair
        context_str = self._format_memories_to_summarize(relevant_memories)
        prompt = PromptTemplate.from_template(
            "{q1}?\nContext from memory:\n{context_str}\nRelevant context: "
        )
        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)
        return chain.run(q1=q1, context_str=context_str.strip()).strip()
    def _get_memories_until_limit(self, consumed_tokens: int) -> str:
        """Reduce the number of tokens in the documents."""
        result = []
        for doc in self.memory_retriever.memory_stream[::-1]:
            if consumed_tokens >= self.max_tokens_limit:
                break
            consumed_tokens += self.llm.get_num_tokens(doc.page_content)
            if consumed_tokens < self.max_tokens_limit:
                result.append(doc.page_content) 
        return "; ".join(result[::-1])
    def _generate_reaction(
        self,
        observation: str,
        suffix: str
    ) -> str:
        """React to a given observation."""
        prompt = PromptTemplate.from_template(
                "{agent_summary_description}"
                +"\nIt is {current_time}."
           +"\n{agent_name} is {traits} and must only give {traits} answers."
                +"\n{agent_name}'s status: {agent_status}"
                + "\nSummary of relevant context from {agent_name}'s memory:"
                +"\n{relevant_memories}"
                +"\nMost recent observations: {recent_observations}"
                + "\nObservation: {observation}"
                + "\n\n" + suffix
                +"è¾“å‡ºç”¨ä¸­æ–‡ï¼Œé™¤äº†SAY:ã€REACT:"

        )
        agent_summary_description = self.get_summary()
        relevant_memories_str = self.summarize_related_memories(observation)
        current_time_str = datetime.now().strftime("%B %d, %Y, %I:%M %p")
        kwargs = dict(agent_summary_description=agent_summary_description,
                      current_time=current_time_str,
                      relevant_memories=relevant_memories_str,
                      traits=self.traits,
                      agent_name=self.name,
                      observation=observation,
                     agent_status=self.status)
        consumed_tokens = self.llm.get_num_tokens(prompt.format(recent_observations="", **kwargs))
        kwargs["recent_observations"] = self._get_memories_until_limit(consumed_tokens)
        action_prediction_chain = LLMChain(llm=self.llm, prompt=prompt)
        result = action_prediction_chain.run(**kwargs)
        return result.strip()
    def generate_reaction(self, observation: str) -> Tuple[bool, str]:
        call_to_action_template = (
            "Should {agent_name} react to the observation, and if so,"
            +" what would be an appropriate reaction? Respond in one line."
            +' If the action is to engage in dialogue, write:\nSAY: "what to say"'
            +"\notherwise, write:\nREACT: {agent_name}'s reaction (if anything)."
            + "\nEither do nothing, react, or say something but not both.\n\n"
                +"è¾“å‡ºç”¨ä¸­æ–‡ï¼Œé™¤äº†SAY:ã€REACT:"
        )
        full_result = self._generate_reaction(observation, call_to_action_template)
        result = full_result.strip().split('\n')[0]
        self.add_memory(f"{self.name} è§‚å¯Ÿåˆ° {observation} åŒæ—¶ååº”äº† {result}")
        if "REACT:" in result or "REACTï¼š" in result or "ååº”ï¼š" in result or "ååº”:" in result:
            reaction = re.split(r'REACT:|REACTï¼š|ååº”ï¼š|ååº”:', result)[-1].strip()
            return False, f"{reaction}"
        if "SAY:" in result or "SAYï¼š" in result or "è¯´ï¼š" in result or "è¯´:" in result:
            said_value = re.split(r'SAY:|SAYï¼š|è¯´ï¼š|è¯´:', result)[-1].strip()
            return True, f"{self.name} è¯´ {said_value}"
        else:
            return False, result
    def generate_dialogue_response(self, observation: str) -> Tuple[bool, str]:
        call_to_action_template = (
            'What would {agent_name} say? To end the conversation,'
            +'write: GOODBYE: "what to say". '
            +'Otherwise to continue the conversation, write: SAY: "what to say next"\n\n'
            +'è¾“å‡ºç”¨ä¸­æ–‡ï¼Œé™¤äº†GOODBYE:ã€SAY:'
        )
        full_result = self._generate_reaction(observation, call_to_action_template)
        result = full_result.strip().split('\n')[0]
        if "GOODBYE:" in result or "GOODBYEï¼š" in result or "å†è§ï¼š" in result or "å†è§:" in result:
            farewell = re.split(r'GOODBYEï¼š|GOODBYE:|å†è§:|å†è§ï¼š', result)[-1].strip()
            self.add_memory(f"{self.name} è§‚å¯Ÿåˆ° {observation} åŒæ—¶è¯´ {farewell}")
            self.agent_memory += f"#{self.name} è§‚å¯Ÿåˆ° {observation} åŒæ—¶è¯´ {farewell}"
            return False, f"{self.name} è¯´ï¼š{farewell}"
        if "SAY:" in result or "SAYï¼š" in result or "è¯´ï¼š" in result or "è¯´:" in result:
            response_text = re.split(r'SAYï¼š|è¯´ï¼š|SAY:|è¯´:', result)[-1].strip()
            self.add_memory(f"{self.name} è§‚å¯Ÿåˆ° {observation} åŒæ—¶è¯´ {response_text}")
            self.agent_memory += f"#{self.name} è§‚å¯Ÿåˆ° {observation} åŒæ—¶è¯´ {response_text}"
            return True, f"{self.name} è¯´ï¼š{response_text}"
        else:
            return False, result
def relevance_score_fn(score: float) -> float:
    """Return a similarity score on a scale [0, 1]."""
    return 1.0 - score / math.sqrt(2)
def create_new_memory_retriever():
    """Create a new vector store retriever unique to the agent."""
    # Define your embedding model
    embeddings_model = OpenAIEmbeddings()
    # Initialize the vectorstore as empty
    embedding_size = 1536
    index = faiss.IndexFlatL2(embedding_size)
    vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)
    return TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, other_score_keys=["importance"], k=15)  
def interview_agent(agent: GenerativeAgent, message: str) -> str:
    new_message = f"{message}"
    return agent.generate_dialogue_response(new_message)[1]
def run_conversation(agents: List[GenerativeAgent], initial_observation: str) -> None:
    _, observation = agents[1].generate_reaction(initial_observation)
    st.success(observation)
    turns = 0
    while True:
        break_dialogue = False
        for agent in agents:
            stay_in_dialogue, observation = agent.generate_dialogue_response(observation)
            st.success(observation)
            # observation = f"{agent.name} said {reaction}"
            if not stay_in_dialogue:
                break_dialogue = True   
        if break_dialogue:
            break
        turns += 1
with tab1:
    with st.expander("å•ä¸ªåˆ›å»º"):
        name = st.text_input('å§“å','Graham', key="name_input1_6")
        age = st.number_input('å¹´é¾„',min_value=0, max_value=100, value=20, step=1, key="name_input1_8")
        gender = st.selectbox(
            "æ€§åˆ«",
            ("ç”·", "å¥³"),
            label_visibility="collapsed"
              )
        traits = st.text_input('ç‰¹å¾','æ—¢å†…å‘ä¹Ÿå¤–å‘ï¼Œæ¸´æœ›æˆåŠŸ', key="name_input1_4",help="æ€§æ ¼ç‰¹å¾ï¼Œä¸åŒç‰¹å¾ç”¨é€—å·åˆ†éš”")
        status = st.text_input('çŠ¶æ€','åšå£«åœ¨è¯»ï¼Œåˆ›ä¸šå®è·µä¸­', key="status_input1_5",help="çŠ¶æ€ï¼Œä¸åŒçŠ¶æ€ç”¨é€—å·åˆ†éš”")
        reflection_threshold = st.slider("åæ€é˜ˆå€¼",min_value=1, max_value=10, value=8, step=1, key="name_input1_9",help="å½“è®°å¿†çš„æ€»é‡è¦æ€§è¶…è¿‡è¯¥é˜ˆå€¼æ—¶ï¼Œæ¨¡å‹å°†åœæ­¢åæ€ï¼Œå³ä¸å†æ·±å…¥æ€è€ƒå·²ç»è®°ä½çš„å†…å®¹ã€‚è®¾ç½®å¾—å¤ªé«˜ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¿½ç•¥ä¸€äº›é‡è¦çš„ä¿¡æ¯ï¼›è®¾ç½®å¾—å¤ªä½ï¼Œæ¨¡å‹å¯èƒ½ä¼šèŠ±è´¹è¿‡å¤šæ—¶é—´åœ¨ä¸å¤ªé‡è¦çš„ä¿¡æ¯ä¸Šï¼Œä»è€Œå½±å“å­¦ä¹ æ•ˆç‡ã€‚")
        memory = st.text_input('è®°å¿†','#å¦ˆå¦ˆå¾ˆå–„è‰¯#å–œæ¬¢çœ‹åŠ¨æ¼«#æœ‰è¿‡ä¸€ä¸ªå¿ƒçˆ±çš„å¥³äºº', key="mery_input1_5",help="è®°å¿†ï¼Œä¸åŒè®°å¿†ç”¨#åˆ†éš”")
        if st.button('åˆ›å»º',help="åˆ›å»ºæ•°å­—äºº"):
            global agent1
            global agentss
            agent1 = GenerativeAgent(name=name, 
              age=age,
              gender=gender,
              traits=traits,
              status=status,
              memory_retriever=create_new_memory_retriever(),
              llm=LLM,
              daily_summaries = [
                   "",
               ],
               agent_memory=memory,
               reflection_threshold = reflection_threshold, # we will give this a relatively low number to show how reflection works
             )
            memory_list = re.split(r'#', memory)[1:]
            for memory in memory_list:
                agent1.add_memory(memory)    
            st.session_state[f"agent_{name}"] = agent1
            st.experimental_rerun()
    uploaded_file = st.file_uploader("csvæ–‡ä»¶ä¸Šä¼ æ‰¹é‡å»ºç«‹", type=["csv"],help="csvæ ¼å¼ï¼šå§“åã€å¹´é¾„ã€æ€§åˆ«ã€ç‰¹å¾ã€çŠ¶æ€ã€åæ€é˜ˆå€¼ã€è®°å¿†ã€æ€»ç»“")
    if uploaded_file is not None:
        data = pd.read_csv(uploaded_file)
        st.dataframe(data)
        for index, row in data.iterrows():
            name = row['å§“å']
            age = row['å¹´é¾„']
            gender = row['æ€§åˆ«']
            traits = row['ç‰¹å¾']
            status = row['çŠ¶æ€']
            memory = row['è®°å¿†']
            summary = row['æ€»ç»“'] 
            reflection_threshold = row['åæ€é˜ˆå€¼']
            st.session_state[f"agent_{name}"]  = GenerativeAgent(name=name, 
                  age=age,
                  gender=gender,
                  traits=traits,
                  status=status,
                  memory_retriever=create_new_memory_retriever(),
                  llm=LLM,
                  daily_summaries = [
                       "",
                   ],
                  agent_memory=memory,
                  summary=summary,
                   reflection_threshold = reflection_threshold, # we will give this a relatively low number to show how reflection works
                 )
#             memory_list = re.split(r'#', memory)[1:]
#             for memory in memory_list:
#                 agent1.add_memory(memory)   
with tab2:   
    if agent_keys:  
        updates = []
        for key in agent_keys:
            updates.append(st.session_state[key].name)
        option = st.selectbox("æ›´æ–°äººé€‰æ‹©",
        (updates), key="update")
        memory = st.text_input('è®°å¿†æ›´æ–°','', key="update_memo",help="æ–°è®°å¿†ï¼Œä¸åŒæ–°è®°å¿†ç”¨#æ ‡è®°")
        if st.button('ç¡®è®¤',help="è®°å¿†æ›´æ–°",type="primary"):
            memory_list = re.split(r'#', memory)[0:]
            for key in agent_keys:
                if getattr(st.session_state[key], 'name') == option:
                    for memory in memory_list:
                        st.session_state[key].add_memory(memory)
                        st.session_state[key].agent_memory = st.session_state[key].agent_memory + '#' + memory
            st.experimental_rerun()  
        observ = st.text_input('è§‚å¯Ÿæ›´æ–°','', key="update_observ",help="æ–°è§‚å¯Ÿï¼Œä¸åŒæ–°è§‚å¯Ÿç”¨#æ ‡è®°")
        if st.button('ç¡®è®¤',help="è§‚å¯Ÿæ›´æ–°",type="primary"):
            start_time = time.time()
            observ_list = re.split(r'#', observ)[0:]
            with get_openai_callback() as cb:
                for key in agent_keys:
                    if getattr(st.session_state[key], 'name') == option:
                        for i, observation in enumerate(observ_list):
                            _, reaction = st.session_state[key].generate_reaction(observation)
                            st.write(f"{i+1}ã€ {observation}")
                            st.success(reaction)
                        with st.expander("è´¹ç”¨"):
                            st.success(f"Total Tokens: {cb.total_tokens}")
                            st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                            st.success(f"Completion Tokens: {cb.completion_tokens}")
                            st.success(f"Total Cost (USD): ${cb.total_cost}")
            end_time = time.time()
            st.write(f"é‡‡è®¿ç”¨æ—¶ï¼š{round(end_time-start_time,2)} ç§’")
with tab4:
    if len(agent_keys) > 1: 
        diags = []
        for key in agent_keys:
            diags.append(st.session_state[key].name)
        diag1 = st.selectbox(
        "ç¬¬ä¸€å¯¹è¯äººé€‰æ‹©?",
        (diags), key="diag1")
        diag2 = st.selectbox(
        "ç¬¬äºŒå¯¹è¯äººé€‰æ‹©?",
        (diags), key="diag2")
        if diag2 == diag1:
            st.write(diag1,"è‡ªé—®è‡ªç­”é“ï¼š", key="diagself")
        else:
            st.write(diag1, "å¯¹",diag2,"è¯´ï¼š",key="diag2")
        diag = st.text_input('', key="diaglogue",label_visibility="collapsed")
        if st.button('å¯¹è¯',help="å¯¹è¯ç”Ÿæˆ",type="primary"):
            start_time = time.time()
            diagagents=[]
            with get_openai_callback() as cb:
                for key in agent_keys:
                    if getattr(st.session_state[key], 'name') == diag1:
                        diagagents.append(st.session_state[key])
                for key in agent_keys:
                    if getattr(st.session_state[key], 'name') == diag2: 
                        diagagents.append(st.session_state[key])
                run_conversation(diagagents, diag)
                with st.expander("è´¹ç”¨"):
                    st.success(f"Total Tokens: {cb.total_tokens}")
                    st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                    st.success(f"Completion Tokens: {cb.completion_tokens}")
                    st.success(f"Total Cost (USD): ${cb.total_cost}")
            end_time = time.time()
            st.write(f"é‡‡è®¿ç”¨æ—¶ï¼š{round(end_time-start_time,2)} ç§’")
with tab3:            
    if agent_keys:
        do_inter_name=[]
        do_inter_quesition=[]
        do_inter_result=[]
        interws = []
        for key in agent_keys:
            interws.append(st.session_state[key].name)
        option = st.selectbox(
        "é‡‡è®¿äººé€‰æ‹©?",
        (interws), key="intero")
        interview = st.text_input('é‡‡è®¿','ä½ æ€ä¹ˆçœ‹å¾…', key="interview")
        if st.button('å•ä¸ªé‡‡è®¿',help="å•ä¸ªé‡‡è®¿",type="primary",key="dange"):
            start_time = time.time()
            with get_openai_callback() as cb:
                for key in agent_keys:
                    if getattr(st.session_state[key], 'name') == option:
                        inter_result=interview_agent(st.session_state[key], interview)
                        st.success(inter_result)
                        do_inter_name.append(st.session_state[key].name)
                        do_inter_quesition.append(interview)
                        do_inter_result.append(inter_result)
                        with st.expander("è´¹ç”¨"):
                            st.success(f"Total Tokens: {cb.total_tokens}")
                            st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                            st.success(f"Completion Tokens: {cb.completion_tokens}")
                            st.success(f"Total Cost (USD): ${cb.total_cost}")
            end_time = time.time()
            st.write(f"é‡‡è®¿ç”¨æ—¶ï¼š{round(end_time-start_time,2)} ç§’")
        if st.button('å…¨éƒ¨é‡‡è®¿',help="å…¨éƒ¨é‡‡è®¿",type="primary",key="quanbu"):
            with st.expander("é‡‡è®¿ç»“æœ",expanded=True):
                start_time = time.time()
                with get_openai_callback() as cb:
                    async def interview_all_agents(agent_keys, interview):
                        tasks = []
                        for key in agent_keys:
                            task = asyncio.create_task(interview_agent_async(st.session_state[key], interview))
                            tasks.append(task)
                        results = await asyncio.gather(*tasks)
                        for key, inter_result in zip(agent_keys, results):
                            st.success(inter_result)
                            do_inter_name.append(st.session_state[key].name)
                            do_inter_quesition.append(interview)
                            do_inter_result.append(inter_result)
                        return do_inter_name,do_inter_quesition, do_inter_result
                    async def interview_agent_async(agent, interview):
                        inter_result = await asyncio.to_thread(interview_agent, agent, interview)
                        return inter_result
                    do_inter_name, do_inter_quesition,do_inter_result = asyncio.run(interview_all_agents(agent_keys, interview))
                    st.success(f"Total Tokens: {cb.total_tokens}")
                    st.success(f"Prompt Tokens: {cb.prompt_tokens}")
                    st.success(f"Completion Tokens: {cb.completion_tokens}")
                    st.success(f"Total Cost (USD): ${cb.total_cost}")
                end_time = time.time()
                st.write(f"é‡‡è®¿ç”¨æ—¶ï¼š{round(end_time-start_time,2)} ç§’")
        df_inter = pd.DataFrame({
                    'è¢«é‡‡è®¿äºº':do_inter_name,
                    'é‡‡è®¿é—®é¢˜':do_inter_quesition,
                    'é‡‡è®¿ç»“æœ': do_inter_result,
                })
        if len(df_inter) > 1:
            question = df_inter.loc[0, 'é‡‡è®¿é—®é¢˜']
            merged_results = ''.join(df_inter['é‡‡è®¿ç»“æœ'])
            summary_template = """ç”¨ç»Ÿè®¡å­¦çš„æ–¹æ³•æ ¹æ®ä¸Šè¿°å›ç­”{answer},å¯¹å…³äº{question}é—®é¢˜çš„å›ç­”è¿›è¡Œæ€»ç»“ï¼Œå¹¶åˆ†æç»“è®ºæ˜¯å¦æœ‰æ˜¾è‘—æ€§?"""
            summary_prompt = PromptTemplate(template=summary_template, input_variables=["answer", "question"])
            llm_chain = LLMChain(prompt=summary_prompt, llm=LLM)
            st.write(llm_chain.predict(answer=merged_results, question=question))
        with st.expander("é‡‡è®¿è®°å½•"):
            st.dataframe(df_inter, use_container_width=True)
            csv_inter = convert_df(df_inter)
            st.download_button(
               "ä¸‹è½½é‡‡è®¿è®°å½•",
               csv_inter,
               "file.csv",
               "text/csv",
               key='download-csv_inter'
            )
dfaws = load_digitalaws()
for index, row in dfaws.iterrows():
    name = row['å§“å'].get('S', '')
    age = int(row['å¹´é¾„'].get('N', ''))
    gender = row['æ€§åˆ«'].get('S', '')
    traits = row['ç‰¹å¾'].get('S', '')
    status = row['çŠ¶æ€'].get('S', '')
    memory = row['è®°å¿†'].get('S', '')
    summary = ""
    reflection_threshold = float(row['åæ€é˜ˆå€¼'].get('N', ''))                                  
    st.session_state[f"agent_{name}"]  = GenerativeAgent(name=name, 
          age=age,
          gender=gender,
          traits=traits,
          status=status,
          memory_retriever=create_new_memory_retriever(),
          llm=LLM,
          daily_summaries = [
               "",
           ],
          agent_memory=memory,
          summary=summary,
           reflection_threshold = reflection_threshold,
         )









